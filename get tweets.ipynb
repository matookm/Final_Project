{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import errno\n",
    "import random as rand\n",
    "import zipfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that loads the twitter API after authorizing the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_api():\n",
    "    consumer_key = 'LaKndoOs7eeWRn5cejLrLsUOM'\n",
    "    consumer_secret = '4be2CdHxazvlxc3BM6NEyZh25yXD2paIzhI81zn61kAgj6VEOV'\n",
    "    access_token = '811597962870669312-qLkkOL256Jyt0RkMqCA70LNV7ocMTRi'\n",
    "    access_secret = 'Xu8ghjy0Q6AoVZtny070REWldgSLH6ZbaqPnAjO1iLYf8'\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    # load the twitter API via tweepy\n",
    "    return tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that takes in a search string 'query', the maximum number of tweets 'max_tweets', and the minimum (i.e., starting)tweet id. It returns a list of tweepy.models.Status objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_search(api, max_tweets, max_id, since_id, geocode):\n",
    "    searched_tweets = []\n",
    "    while len(searched_tweets) < max_tweets:\n",
    "        remaining_tweets = max_tweets - len(searched_tweets)\n",
    "        try:\n",
    "            new_tweets = api.search(q='*',\n",
    "                                    count=remaining_tweets, \n",
    "                                    since_id=str(since_id),\n",
    "                                    lang='en', \n",
    "                                    max_id=str(max_id-1))\n",
    "            #print 'found',len(new_tweets),'tweets'\n",
    "            if not new_tweets:\n",
    "                print 'no tweets found'\n",
    "                break\n",
    "            searched_tweets.extend(new_tweets)\n",
    "            max_id = new_tweets[-1].id\n",
    "        except tweepy.TweepError:\n",
    "            print 'exception raised, waiting 15 minutes'\n",
    "            print '(until:', dt.datetime.now()+dt.timedelta(minutes=15), ')'\n",
    "            time.sleep(15*60)\n",
    "            break # stop the loop\n",
    "    return searched_tweets, max_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that gets the ID of a tweet. This ID can then be used as a 'starting point' from which to search. The query is required and has been set to a commonly used word by default.\n",
    "The variable 'days_ago' has been initialized to the maximum amount we are able to search back in time (9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tweet_id(api, date='', days_ago=9, query='a'):\n",
    "    if date:\n",
    "        # return an ID from the start of the given day\n",
    "        td = date + dt.timedelta(day=1)\n",
    "        tweet_date = '{0}-{1:0>2}-{2:0>2}'.format(td.year, td.month, td.day)\n",
    "        tweet = api.search(q=query, count=1, until=tweet_date)\n",
    "    else:\n",
    "        # return an ID from __ days ago\n",
    "        td = dt.datetime.now() - dt.timedelta(days=days_ago)\n",
    "        tweet_date = '{0}-{1:0>2}-{2:0>2}'.format(td.year, td.month, td.day)\n",
    "        # get list of up to 10 tweets\n",
    "        tweet = api.search(q=query, count=10, until=tweet_date)\n",
    "        print('search limit (start/stop):',tweet[0].created_at)\n",
    "        # return the id of the first tweet in the list\n",
    "        return tweet[0].id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that appends tweets to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_tweets(tweets, filename):\n",
    "    with open(filename, 'a') as f:\n",
    "        for tweet in tweets:\n",
    "            json.dump(tweet._json, f)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def catch_tweets(counter):\n",
    "    ''' This is a script that continuously searches for tweets\n",
    "        that were created over a given number of days. The search\n",
    "        dates and search phrase can be changed below. '''\n",
    "\n",
    "\n",
    "\n",
    "    # search variables:\n",
    "    time_limit = 1.2                           # runtime limit in hours\n",
    "    max_tweets = 100                           # number of tweets per search (will be iterated over) - maximum is 100\n",
    "    min_days_old, max_days_old = 6, 7          # search limits e.g., from 7 to 8 gives current weekday from last week,\n",
    "                                               # min_days_old=0 will search from right now\n",
    "    USA = '39.8,-95.583068847656,2500km'       # this geocode includes nearly all American\n",
    "                                               # states (and a large portion of Canada)\n",
    "    \n",
    "    \n",
    "    # open a file in which to store the tweets\n",
    "    if max_days_old - min_days_old == 1:\n",
    "        d = dt.datetime.now() - dt.timedelta(days=min_days_old)\n",
    "        day = '{0}-{1:0>2}-{2:0>2}'.format(d.year, d.month, d.day)\n",
    "        day_and_hour = '{0}-{1:0>2}-{2:0>2}-{3:0>2}-{4:0>2}'.format(d.year, d.month, d.day,d.hour,d.minute)\n",
    "    else:\n",
    "        d1 = dt.datetime.now() - dt.timedelta(days=max_days_old-1)\n",
    "        d2 = dt.datetime.now() - dt.timedelta(days=min_days_old)\n",
    "        day = '{0}-{1:0>2}-{2:0>2}'.format(d1.year, d1.month, d1.day)\n",
    "        day_and_hour = '{0}-{1:0>2}-{2:0>2}-{3:0>2}{4:0>2}_to_{5:0>2}-{6:0>2}-{7:0>2}-{8:0>2}{9:0>2}'.format(\n",
    "              d1.year, d1.month, d1.day,d1.hour,d1.minute, d2.year, d2.month, d2.day,d2.hour,d2.minute)\n",
    "    \n",
    "    # loop over search items, creating a new file for each\n",
    "    global json_root\n",
    "    json_root = \"All_Tweets_\" + day\n",
    "    json_file_root = json_root + '/'  \n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(json_file_root))\n",
    "    except OSError as exc:\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise exc\n",
    "        pass\n",
    "    read_IDs = False\n",
    "    \n",
    "    json_file = json_file_root + 'All_Tweets_' + day_and_hour + '.json'\n",
    "    if os.path.isfile(json_file):\n",
    "        print('Appending tweets to file named: ',json_file)\n",
    "        read_IDs = True\n",
    "\n",
    "    # authorize and load the twitter API\n",
    "    api = load_api()\n",
    "    # set the smallest ID to search for\n",
    "    since_id = get_tweet_id(api, days_ago=(max_days_old-1))\n",
    "    # set the 'starting point' ID for tweet collection\n",
    "    if read_IDs:\n",
    "        # open the json file and get the latest tweet ID\n",
    "        with open(json_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            max_id = json.loads(lines[-1])['id']\n",
    "            print('Searching from the bottom ID in file')\n",
    "    else:\n",
    "        # get the ID of a tweet that is min_days_old\n",
    "        if min_days_old == 0:\n",
    "            max_id = -1\n",
    "        else:\n",
    "            to_change_time = 181193930712960 * counter/10\n",
    "            max_id = get_tweet_id(api, days_ago=(min_days_old-1)) - to_change_time\n",
    "            print 'time interval = ', to_change_time\n",
    "    print 'max id (starting point) = ', max_id\n",
    "    print 'since id (ending point) = ', since_id\n",
    "\n",
    "    ''' tweet gathering loop  '''\n",
    "    start = dt.datetime.now()\n",
    "    end = start + dt.timedelta(hours=time_limit)\n",
    "    count, exitcount = 0, 0\n",
    "    while dt.datetime.now() < end:\n",
    "        count += 1\n",
    "        # collect tweets and update max_id\n",
    "        tweets, max_id = tweet_search(api, max_tweets,\n",
    "                                      max_id=max_id, \n",
    "                                      since_id=since_id,\n",
    "                                      geocode=USA)\n",
    "        # write tweets to file in JSON format\n",
    "        if tweets:\n",
    "            write_tweets(tweets, json_file)\n",
    "            exitcount = 0\n",
    "        else:\n",
    "            exitcount += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zip all the files together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zip(src, dst):\n",
    "    zf = zipfile.ZipFile(\"%s.zip\" % (dst), \"w\", zipfile.ZIP_DEFLATED)\n",
    "    abs_src = os.path.abspath(src)\n",
    "    for dirname, subdirs, files in os.walk(src):\n",
    "        for filename in files:\n",
    "            absname = os.path.abspath(os.path.join(dirname, filename))\n",
    "            arcname = absname[len(abs_src) + 1:]\n",
    "            print 'zipping %s as %s' % (os.path.join(dirname, filename),arcname)\n",
    "            zf.write(absname, arcname)\n",
    "    zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time:  2017-05-29 09:19:26.261000\n",
      "startig loop  1\n",
      "('search limit (start/stop):', datetime.datetime(2017, 5, 22, 23, 59, 59))\n",
      "('search limit (start/stop):', datetime.datetime(2017, 5, 23, 23, 59, 59))\n",
      "time interval =  18119393071296\n",
      "max id (starting point) =  867150144579751745\n",
      "since id (ending point) =  866805876123762690\n",
      "exception raised, waiting 15 minutes\n",
      "(until: 2017-05-29 09:39:22.710000 )\n",
      "exception raised, waiting 15 minutes\n",
      "(until: 2017-05-29 09:59:06.757000 )\n",
      "exception raised, waiting 15 minutes\n",
      "(until: 2017-05-29 10:18:49.452000 )\n",
      "exception raised, waiting 15 minutes\n",
      "(until: 2017-05-29 10:38:31.400000 )\n",
      "startig loop  3\n",
      "('search limit (start/stop):', datetime.datetime(2017, 5, 22, 23, 59, 59))\n",
      "('search limit (start/stop):', datetime.datetime(2017, 5, 23, 23, 59, 59))\n",
      "time interval =  54358179213888\n",
      "max id (starting point) =  867113905793609153\n",
      "since id (ending point) =  866805876123762690\n",
      "exception raised, waiting 15 minutes\n",
      "(until: 2017-05-29 10:58:14.905000 )\n",
      "exception raised, waiting 15 minutes\n",
      "(until: 2017-05-29 11:15:24.573000 )\n",
      "exception raised, waiting 15 minutes\n",
      "(until: 2017-05-29 11:35:18.647000 )\n",
      "exception raised, waiting 15 minutes\n",
      "(until: 2017-05-29 11:55:07.146000 )\n",
      "startig loop  5\n",
      "('search limit (start/stop):', datetime.datetime(2017, 5, 22, 23, 59, 59))\n",
      "('search limit (start/stop):', datetime.datetime(2017, 5, 23, 23, 59, 59))\n",
      "time interval =  90596965356480\n",
      "max id (starting point) =  867077667007466561\n",
      "since id (ending point) =  866805876123762690\n",
      "exception raised, waiting 15 minutes\n",
      "(until: 2017-05-29 12:15:04.591000 )\n",
      "exception raised, waiting 15 minutes\n",
      "(until: 2017-05-29 12:35:18.712000 )\n",
      "exception raised, waiting 15 minutes\n",
      "(until: 2017-05-29 12:55:14.320000 )\n",
      "exception raised, waiting 15 minutes\n",
      "(until: 2017-05-29 13:14:55.750000 )\n",
      "startig loop  7\n",
      "('search limit (start/stop):', datetime.datetime(2017, 5, 22, 23, 59, 59))\n",
      "('search limit (start/stop):', datetime.datetime(2017, 5, 23, 23, 59, 59))\n",
      "time interval =  126835751499072\n",
      "max id (starting point) =  867041428221323969\n",
      "since id (ending point) =  866805876123762690\n",
      "exception raised, waiting 15 minutes\n",
      "(until: 2017-05-29 13:34:43.208000 )\n",
      "no tweets found\n",
      "exception raised, waiting 15 minutes\n",
      "(until: 2017-05-29 13:54:33.650000 )\n",
      "exception raised, waiting 15 minutes\n",
      "(until: 2017-05-29 14:14:21.352000 )\n",
      "exception raised, waiting 15 minutes\n",
      "(until: 2017-05-29 14:34:10.760000 )\n",
      "startig loop  9\n",
      "('search limit (start/stop):', datetime.datetime(2017, 5, 22, 23, 59, 59))\n",
      "('search limit (start/stop):', datetime.datetime(2017, 5, 23, 23, 59, 59))\n",
      "time interval =  163074537641664\n",
      "max id (starting point) =  867005189435181377\n",
      "since id (ending point) =  866805876123762690\n",
      "exception raised, waiting 15 minutes\n",
      "(until: 2017-05-29 14:54:00.675000 )\n",
      "exception raised, waiting 15 minutes\n",
      "(until: 2017-05-29 15:14:27.534000 )\n",
      "exception raised, waiting 15 minutes\n",
      "(until: 2017-05-29 15:34:51.350000 )\n",
      "exception raised, waiting 15 minutes\n",
      "(until: 2017-05-29 15:55:08.966000 )\n",
      "zipping All_Tweets_2017-05-23\\All_Tweets_2017-05-23-09-19.json as All_Tweets_2017-05-23-09-19.json\n",
      "zipping All_Tweets_2017-05-23\\All_Tweets_2017-05-23-10-38.json as All_Tweets_2017-05-23-10-38.json\n",
      "zipping All_Tweets_2017-05-23\\All_Tweets_2017-05-23-11-55.json as All_Tweets_2017-05-23-11-55.json\n",
      "zipping All_Tweets_2017-05-23\\All_Tweets_2017-05-23-13-14.json as All_Tweets_2017-05-23-13-14.json\n",
      "zipping All_Tweets_2017-05-23\\All_Tweets_2017-05-23-14-34.json as All_Tweets_2017-05-23-14-34.json\n",
      "end time:  2017-05-29 15:55:54.245000\n",
      "stopped   2017-05-29 15:55:54.245000\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    for i in range(1,10,2):\n",
    "        print \"startig loop \", i\n",
    "        catch_tweets(i)\n",
    "    zip(json_root, \"zip_\" + json_root)\n",
    "    shutil.rmtree(json_root)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print 'start time: ',dt.datetime.now()\n",
    "    main()\n",
    "    print 'end time: ',dt.datetime.now()\n",
    "print 'stopped  ',dt.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
