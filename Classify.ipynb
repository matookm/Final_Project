{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn import neighbors \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pickle\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import xlsxwriter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from nltk.tokenize import word_tokenize\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paths to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_path = r'C:\\Users\\matoo\\Documents\\4th year\\final project\\some code\\Results\\Top featurs\\\\'\n",
    "read_directory = r'C:\\Users\\matoo\\Documents\\4th year\\final project\\some code\\Pickles\\concat\\test\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns the top features for every model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_top_features(classifier, fileName, str):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    dfSignificantFeatures = pd.DataFrame()\n",
    "    class_labels = classifier.lables_\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        top20 = np.argsort(classifier.coef_[i])[-20:]\n",
    "        toList = (\" %s\" % (\n",
    "              \",\".join(feature_names[j] for j in top20)))\n",
    "        list = toList.split(sep=',')\n",
    "\n",
    "        print(\"%s: %s\" % (class_label,\n",
    "                          \",\".join(feature_names[j] for j in top20)))\n",
    "        dfSignificantFeatures[class_label.upper()] = list[::-1]\n",
    "    dfSignificantFeatures.to_csv(write_path+fileName + '_best10Features_'+str+'.csv', mode='a', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_occurencens(df, feature_names):\n",
    "    langCorpus = np.array(df.Text.values).tolist()\n",
    "    #tokenize words in tweet\n",
    "    tokenized_tweets = [word_tokenize(i) for i in langCorpus]\n",
    "    #merge all tweets\n",
    "    allwords = [item for sublist in tokenized_tweets for item in sublist]\n",
    "    allTokens= (itertools.chain.from_iterable(tokenized_tweets))\n",
    "    resultwords = [word for word in allTokens if word in feature_names]\n",
    "    tokenFreq = nltk.FreqDist(resultwords)\n",
    "    ans = len(resultwords)/float(len(allwords))\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_test_set(df, numOfFeatures, RemoveStopWords, n, analyzer):\n",
    "    stops = set(stopwords.words('english'))\n",
    "    corpus = np.array(df.Text.values).tolist()\n",
    "    target = np.array(df['User Language'].values)\n",
    "    if RemoveStopWords:\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', max_features=numOfFeatures, analyzer=analyzer, ngram_range=(n, n))\n",
    "    else: #only SW\n",
    "        vectorizer = TfidfVectorizer(vocabulary=stops ,max_features=numOfFeatures, analyzer=analyzer, use_idf=False, ngram_range=(n, n))\n",
    "    \n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    global feature_names\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    return X, target, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function plots a confusion matrix to a .png file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_cnf_matrix(cms, classes, fileName, model_name):\n",
    "    df_cm = pd.DataFrame(cms, index = classes, columns = classes)\n",
    "    plt.figure(figsize = (100,100))\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = sn.heatmap(df_cm, annot=True, linewidths=.5, fmt='g')\n",
    "    fig.savefig(write_path + fileName + '_cm_' + model_name +'.png', format='png')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This function helps us to classify our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Classify(df,fileName,RemoveStopWords,analyzer,n):\n",
    "    a = {'Classifer': pd.Series(index=['DT', 'Naive bayes' ,'SVM linear']),} #, '% total occur'\n",
    "    scoreTable = pd.DataFrame(a)\n",
    "\n",
    "    scoreTable.__delitem__('Classifer')\n",
    "\n",
    "    tests = ['allLangs'] \n",
    "    nums = [500, 750, 1000, 1500, 2000, 3000, 5000, 7000]\n",
    "\n",
    "    for test in tests:\n",
    "        dfReduced = df.copy(deep=True)\n",
    "\n",
    "        if test is not \"allLangs\":\n",
    "            langs = str.split(test, '_')\n",
    "            dfReduced = dfReduced[dfReduced['Lang'].isin(langs)]\n",
    "        \n",
    "        #if we want to check for several x top featurs\n",
    "        for numOfFeatures in nums:\n",
    "            nfolds=5\n",
    "            accuracies = []\n",
    "            file_name = fileName + '_' + str(numOfFeatures) #if we need to change the output file name\n",
    "            data, target, feat_names = extract_test_set(dfReduced,numOfFeatures,RemoveStopWords, n, analyzer)\n",
    "            kf = KFold(data.shape[0], n_folds = nfolds, shuffle = True, random_state = 1)\n",
    "            \n",
    "            test = test+\" top \"+str(numOfFeatures)+\" features\"\n",
    "            \n",
    "            dtree = DecisionTreeClassifier(random_state=0, max_depth=50)\n",
    "            acc = train_model_kf_cv(dtree, kf, data, target, nfolds, file_name, 'DT')\n",
    "            accuracies.append(acc) \n",
    "            \n",
    "            bayes = nb.MultinomialNB()\n",
    "            acc = train_model_kf_cv(bayes, kf, data, target, nfolds, file_name, 'Naive bayes')\n",
    "            accuracies.append(acc)\n",
    "            \n",
    "            svm = SVC(decision_function_shape='ovo',kernel='linear')\n",
    "            acc = train_model_kf_cv(svm, kf, data, target, nfolds, file_name, 'SVM linear')\n",
    "            accuracies.append(acc)\n",
    "            \n",
    "            #knn = neighbors.KNeighborsClassifier()\n",
    "            #acc = train_model_kf_cv(knn, kf, data, target, nfolds, fileName, 'KNN')\n",
    "            #accuracies.append(acc)\n",
    "            \n",
    "            #occ = calculate_occurencens(df, feat_names)\n",
    "            #accuracies.append(occ)\n",
    "            \n",
    "\n",
    "            scoreTable[file_name] = accuracies\n",
    "            \n",
    "    writer = pd.ExcelWriter(write_path + file_name + '_Results.xlsx') \n",
    "    scoreTable.to_excel(writer,'Score table')\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function trains the model k-fold times and returns total accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model_kf_cv(model, kf, data, target, numFolds, fileName, model_name):\n",
    "    cm = []\n",
    "    error = []\n",
    "    for train_indices, test_indices in kf:\n",
    "        # Get the dataset; this is the way to access values in a pandas DataFrame\n",
    "        train_X = data[train_indices, :]\n",
    "        train_Y = target[train_indices]\n",
    "        test_X = data[test_indices, :]\n",
    "        test_Y = target[test_indices]\n",
    "        # Train the model\n",
    "        model.fit(train_X, train_Y)\n",
    "        predictions = model.predict(test_X)\n",
    "        # Evaluate the model\n",
    "        classes = model.classes_                \n",
    "        cm.append(confusion_matrix(test_Y, predictions, labels=classes))\n",
    "        ###fpr, tpr, _ = roc_curve(test_Y, predictions)\n",
    "        ###total += auc(fpr, tpr)\n",
    "        error.append(model.score(test_X, test_Y))\n",
    "    accuracy = np.mean(error)\n",
    "    for i in range(0,9):\n",
    "        cms = np.mean(cm, axis=0)\n",
    "    plot_cnf_matrix(cms, classes, fileName, model_name)\n",
    "    ###auc = total / numFolds\n",
    "    ###print \"AUC of {0}: {1}\".format(Model.__name__, accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: concat_1 num: 1 out of 2\n"
     ]
    }
   ],
   "source": [
    "lang10 =['bg' 'cs' 'de' 'es' 'fr' 'it' 'ja' 'ko' 'nl' 'ru']\n",
    "maxsall=[]\n",
    "\n",
    "pickles = os.listdir(read_directory)\n",
    "b = {'Data': pd.Series(index=[pickles]),}\n",
    "SummerayTable = pd.DataFrame(b)\n",
    "\n",
    "SummerayTable.__delitem__('Data')\n",
    "\n",
    "count=1\n",
    "for file in pickles:\n",
    "    print (\"file: \"+ file+\" num: \"+str(count)+\" out of \" +str(len(pickles)))\n",
    "    count=count+1\n",
    "    df = pd.read_pickle(read_directory + str(file))\n",
    "    feature_names = list(df.columns.values)\n",
    "    Classify(df, file, True, 'word', 1)  #word or char if character\n",
    "    #Classify(df, file, True, 'char', 3)  #word or char if character"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
